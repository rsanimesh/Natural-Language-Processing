{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-processing using nltk and pyspellchecker\n",
    "\n",
    "This notebook contains code for text preprocessing using NLTK and spellchecker.\n",
    "\n",
    "#### Steps for Text Pre-Processing\n",
    " 0. Split sentences\n",
    " 1. Tokenize sentences\n",
    " 2. Spell Check\n",
    " 3. Part of Speech Tagging or Pos Tagging\n",
    " 4. Stop-words removal\n",
    " 5. Lowercase\n",
    " 6. Non Alpha/ Alpha Numeric characters removal\n",
    " 7. Stemming and Lemmatization\n",
    "\n",
    "NLTK is standard and one of the most popular library in python for text data\n",
    ". And pyspellchecker is simple easy to use library for spellcheck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    " Below are installation instruction of libraries requeried for this notebook\n",
    "\n",
    ">* `!pip install pandas`\n",
    "* `!pip install pyspellchecker`\n",
    "* `!pip  install nltk`\n",
    "* `nltk.download('punkt')`\n",
    "* `nltk.download('averaged_perceptron_tagger')`\n",
    "* `nltk.download('stopwords')`\n",
    "* `nltk.download('wordnet')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import pandas as pd\n",
    "\n",
    "# nltk imports used in preprocessing\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# import for spellcheck\n",
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = [\"i want to travel. Hence book my tickets.\",\n",
    "            \"please book flight tickets for tomorow morning.\",\n",
    "            \"the shwo was vry good. but it was very costly.\",\n",
    "            \"let's complete wrk next weekend. So that we can have party afterwards.\",\n",
    "            \"my work is allmost complted\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i want to travel.',\n",
       " 'Hence book my tickets.',\n",
       " 'please book flight tickets for tomorow morning.',\n",
       " 'the shwo was vry good.',\n",
       " 'but it was very costly.',\n",
       " \"let's complete wrk next weekend.\",\n",
       " 'So that we can have party afterwards.',\n",
       " 'my work is allmost complted']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = sum([sent_tokenize(text) for text in text_data],[])\n",
    "text_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize sentences to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'want', 'to', 'travel', '.'], ['Hence', 'book', 'my', 'tickets', '.'], ['please', 'book', 'flight', 'tickets', 'for', 'tomorow', 'morning', '.'], ['the', 'shwo', 'was', 'vry', 'good', '.'], ['but', 'it', 'was', 'very', 'costly', '.'], ['let', \"'s\", 'complete', 'wrk', 'next', 'weekend', '.'], ['So', 'that', 'we', 'can', 'have', 'party', 'afterwards', '.'], ['my', 'work', 'is', 'allmost', 'complted']]\n"
     ]
    }
   ],
   "source": [
    "sentences = [word_tokenize(text) for text in text_data]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spellcheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'allmost': 'almost',\n",
       " 'shwo': 'show',\n",
       " \"'s\": 'is',\n",
       " 'vry': 'very',\n",
       " 'wrk': 'work',\n",
       " 'complted': 'completed',\n",
       " 'tomorow': 'tomorrow'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initializing pyspellchecker SpellChecker obj\n",
    "spell = SpellChecker()\n",
    "\n",
    "# finidng out misspelled words\n",
    "misspelled = spell.unknown(sum(sentences,[]))\n",
    "\n",
    "# finding correct word and storing in dictionary\n",
    "misspelled_dict = dict((word,spell.correction(word)) for word in misspelled)\n",
    "misspelled_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'want', 'to', 'travel', '.'],\n",
       " ['Hence', 'book', 'my', 'tickets', '.'],\n",
       " ['please', 'book', 'flight', 'tickets', 'for', 'tomorrow', 'morning', '.'],\n",
       " ['the', 'show', 'was', 'very', 'good', '.'],\n",
       " ['but', 'it', 'was', 'very', 'costly', '.'],\n",
       " ['let', 'is', 'complete', 'work', 'next', 'weekend', '.'],\n",
       " ['So', 'that', 'we', 'can', 'have', 'party', 'afterwards', '.'],\n",
       " ['my', 'work', 'is', 'almost', 'completed']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replacing spelling mistakes in all sentences\n",
    "for word in misspelled_dict.keys():\n",
    "    for sentence in sentences:\n",
    "        try:\n",
    "            sentence[sentence.index(word)] = misspelled_dict[word]\n",
    "        except ValueError:\n",
    "            pass\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging or Pos Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('i', 'NN'), ('want', 'VBP'), ('to', 'TO'), ('travel', 'VB'), ('.', '.')], [('Hence', 'NNP'), ('book', 'NN'), ('my', 'PRP$'), ('tickets', 'NNS'), ('.', '.')], [('please', 'VB'), ('book', 'NN'), ('flight', 'NN'), ('tickets', 'NNS'), ('for', 'IN'), ('tomorrow', 'NN'), ('morning', 'NN'), ('.', '.')], [('the', 'DT'), ('show', 'NN'), ('was', 'VBD'), ('very', 'RB'), ('good', 'JJ'), ('.', '.')], [('but', 'CC'), ('it', 'PRP'), ('was', 'VBD'), ('very', 'RB'), ('costly', 'JJ'), ('.', '.')], [('let', 'NN'), ('is', 'VBZ'), ('complete', 'JJ'), ('work', 'NN'), ('next', 'JJ'), ('weekend', 'NN'), ('.', '.')], [('So', 'RB'), ('that', 'IN'), ('we', 'PRP'), ('can', 'MD'), ('have', 'VB'), ('party', 'NN'), ('afterwards', 'NNS'), ('.', '.')], [('my', 'PRP$'), ('work', 'NN'), ('is', 'VBZ'), ('almost', 'RB'), ('completed', 'VBN')]]\n"
     ]
    }
   ],
   "source": [
    "pos_tagged = [nltk.pos_tag(sentence) for sentence in sentences]\n",
    "print(pos_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['want', 'travel', '.'],\n",
       " ['Hence', 'book', 'tickets', '.'],\n",
       " ['please', 'book', 'flight', 'tickets', 'tomorrow', 'morning', '.'],\n",
       " ['show', 'good', '.'],\n",
       " ['costly', '.'],\n",
       " ['let', 'complete', 'work', 'next', 'weekend', '.'],\n",
       " ['So', 'party', 'afterwards', '.'],\n",
       " ['work', 'almost', 'completed']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk comes with a predefined list of stopwords,\n",
    "# we can add or remove words from it according to out application need\n",
    "# many a times we do not want to remove few words like wh-question words\n",
    "# And other words like 'not' as they may result in information loss\n",
    "\n",
    "words_not_to_reomve = [\"not\", \"where\", \"why\", \"how\", \"what\", \"who\", \"which\", \"when\", \"whom\"]\n",
    "stopwords_list = [words for words in stopwords.words('english') if words not in words_not_to_reomve]\n",
    "\n",
    "sentences_without_stopwords = sentences.copy()\n",
    "\n",
    "for sentence in sentences_without_stopwords:\n",
    "    intersection = set(sentence).intersection(set(stopwords_list))\n",
    "    if intersection:\n",
    "        for w in intersection:\n",
    "            sentence.remove(w)\n",
    "            \n",
    "sentences_without_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercase sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['want', 'travel', '.'],\n",
       " ['hence', 'book', 'tickets', '.'],\n",
       " ['please', 'book', 'flight', 'tickets', 'tomorrow', 'morning', '.'],\n",
       " ['show', 'good', '.'],\n",
       " ['costly', '.'],\n",
       " ['let', 'complete', 'work', 'next', 'weekend', '.'],\n",
       " ['so', 'party', 'afterwards', '.'],\n",
       " ['work', 'almost', 'completed']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_lowercase = [[words.lower() for words in sent] for sent in sentences]\n",
    "sentences_lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non Alpha/ Alpha Numeric characters removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['want', 'travel'],\n",
       " ['Hence', 'book', 'tickets'],\n",
       " ['please', 'book', 'flight', 'tickets', 'tomorrow', 'morning'],\n",
       " ['show', 'good'],\n",
       " ['costly'],\n",
       " ['let', 'complete', 'work', 'next', 'weekend'],\n",
       " ['So', 'party', 'afterwards'],\n",
       " ['work', 'almost', 'completed']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keeping only alpha numeric chatracters\n",
    "# if you want numbers too use isalnum() instead of .isalpha()\n",
    "\n",
    "sentences_alpha = [[word for word in sent if word.isalpha()] for sent in sentences]\n",
    "sentences_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and lemmatization\n",
    "\n",
    "Here is the definition from wikipedia for stemming and lemmatization:\n",
    "\n",
    "> Stemming is the process for reducing inflected (or sometimes derived) words to their stem, base or root form—generally a written word form\n",
    "\n",
    "> Lemmatisation (or lemmatization) in linguistics, is the process of grouping together the different inflected forms of a word so they can be analysed as a single item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['python', 'python', 'python', 'python', 'pythonli']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming Example\n",
    "\n",
    "ps = PorterStemmer()\n",
    "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
    "[ps.stem(w) for w in example_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\n",
      "best\n"
     ]
    }
   ],
   "source": [
    "# Lematizing Example\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"was\", pos=\"v\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['want', 'travel'],\n",
       " ['henc', 'book', 'ticket'],\n",
       " ['pleas', 'book', 'flight', 'ticket', 'tomorrow', 'morn'],\n",
       " ['show', 'good'],\n",
       " ['costli'],\n",
       " ['let', 'complet', 'work', 'next', 'weekend'],\n",
       " ['So', 'parti', 'afterward'],\n",
       " ['work', 'almost', 'complet']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_stemed = [[ps.stem(word) for word in sent] for sent in sentences_alpha]\n",
    "sentences_stemed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['want', 'travel'],\n",
       " ['henc', 'book', 'ticket'],\n",
       " ['plea', 'book', 'flight', 'ticket', 'tomorrow', 'morn'],\n",
       " ['show', 'good'],\n",
       " ['costli'],\n",
       " ['let', 'complet', 'work', 'next', 'weekend'],\n",
       " ['So', 'parti', 'afterward'],\n",
       " ['work', 'almost', 'complet']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_lematized = [[lemmatizer.lemmatize(word) for word in sent] for sent in sentences_stemed]\n",
    "sentences_lematized"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
